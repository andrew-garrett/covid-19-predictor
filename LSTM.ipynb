{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8s+l+nc0x3ZFr9LnjQWgs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geande/covid-19-predictor/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aeo-mw2VVpFc"
      },
      "source": [
        "import pandas\r\n",
        "import jax.numpy as np\r\n",
        "from jax import random, vmap, grad, jit\r\n",
        "from jax.experimental import optimizers\r\n",
        "from jax.ops import index_update, index\r\n",
        "\r\n",
        "import itertools\r\n",
        "from functools import partial\r\n",
        "from tqdm import trange\r\n",
        "import numpy.random as npr\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from mpl_toolkits.mplot3d import Axes3D\r\n",
        "from scipy.interpolate import griddata\r\n",
        "from scipy.integrate import odeint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEVyrkfSVgdq"
      },
      "source": [
        "# Create lags function, nothing changes here\r\n",
        "def create_lags(data, L):\r\n",
        "    N = data.shape[0] - L\r\n",
        "    D = data.shape[1]\r\n",
        "    X = np.zeros((L, N, D))\r\n",
        "    Y = np.zeros((N, D))\r\n",
        "    for i in range(0,N):\r\n",
        "        X = index_update(X, index[:,i,:], data[i:(i+L), :])\r\n",
        "        Y = index_update(Y, index[i,:], data[i+L, :])\r\n",
        "    return X, Y\r\n",
        "\r\n",
        "# Define our logistic sigmoid which we will use later on\r\n",
        "def sigmoid(x):\r\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkCUMJKNVll1",
        "outputId": "fe793a1d-dded-44f2-ce12-094f42ac02e0"
      },
      "source": [
        "class RNN():\r\n",
        "  def __init__(self, dataset, num_lags, hidden_dim, rng_key = random.PRNGKey(0)):\r\n",
        "    # Normalize across data-points dimension\r\n",
        "    self.mean, self.std = dataset.mean(0), dataset.std(0)\r\n",
        "    dataset = (dataset - self.mean)/self.std\r\n",
        "\r\n",
        "    # Create the lagged normalized training data\r\n",
        "    # X: L x N x D\r\n",
        "    # Y: N x D\r\n",
        "    self.X, self.Y = create_lags(dataset, num_lags)\r\n",
        "    self.X_dim = self.X.shape[-1]\r\n",
        "    self.Y_dim = self.Y.shape[-1]\r\n",
        "    self.hidden_dim = hidden_dim\r\n",
        "    self.num_lags = num_lags\r\n",
        "\r\n",
        "    # Initialization and evaluation functions\r\n",
        "    self.net_init, self.net_apply = self.init_RNN()\r\n",
        "    \r\n",
        "    # Initialize parameters, not committing to a batch shape\r\n",
        "    self.net_params = self.net_init(rng_key)\r\n",
        "                \r\n",
        "    # Use optimizers to set optimizer initialization and update functions\r\n",
        "    self.opt_init, \\\r\n",
        "    self.opt_update, \\\r\n",
        "    self.get_params = optimizers.adam(1e-1)\r\n",
        "    self.opt_state = self.opt_init(self.net_params)\r\n",
        "\r\n",
        "    # Logger to monitor the loss function\r\n",
        "    self.loss_log = []\r\n",
        "    self.itercount = itertools.count()\r\n",
        "\r\n",
        "  def init_RNN(self):\r\n",
        "    # Define init function\r\n",
        "    def _init(rng_key):\r\n",
        "        # Define methods for initializing the weights\r\n",
        "        def glorot_normal(rng_key, size):\r\n",
        "          in_dim = size[0]\r\n",
        "          out_dim = size[1]\r\n",
        "          glorot_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\r\n",
        "          return glorot_stddev*random.normal(rng_key, (in_dim, out_dim))\r\n",
        "\r\n",
        "        # Inputs\r\n",
        "        Uo = glorot_normal(rng_key, (self.X_dim, self.hidden_dim))\r\n",
        "        Us = glorot_normal(rng_key, (self.X_dim, self.hidden_dim))\r\n",
        "        Ui = glorot_normal(rng_key, (self.X_dim, self.hidden_dim))\r\n",
        "        Uf = glorot_normal(rng_key, (self.X_dim, self.hidden_dim))\r\n",
        "\r\n",
        "        # Biases all initialized to 0\r\n",
        "        bo = np.zeros(self.hidden_dim)\r\n",
        "        bs = np.zeros(self.hidden_dim)\r\n",
        "        bi = np.zeros(self.hidden_dim)\r\n",
        "        bf = np.zeros(self.hidden_dim)\r\n",
        "\r\n",
        "        # Transition dynamics\r\n",
        "        Wo = np.eye(self.hidden_dim)\r\n",
        "        Ws = np.eye(self.hidden_dim)\r\n",
        "        Wi = np.eye(self.hidden_dim)\r\n",
        "        Wf = np.eye(self.hidden_dim)\r\n",
        "\r\n",
        "        # Outputs\r\n",
        "        V = glorot_normal(rng_key, (self.hidden_dim, self.Y_dim))\r\n",
        "        c = np.zeros(self.Y_dim)\r\n",
        "\r\n",
        "        return (Uo, Us, Ui, Uf, bo, bs, bi, bf, Wo, Ws, Wi, Wf, V, c)\r\n",
        "    # Define apply function\r\n",
        "    def _apply(params, input):\r\n",
        "        Uo, Us, Ui, Uf, bo, bs, bi, bf, Wo, Ws, Wi, Wf, V, c = params\r\n",
        "        H = np.zeros((input.shape[1], self.hidden_dim))\r\n",
        "        s_t = np.zeros((input.shape[1], self.hidden_dim))\r\n",
        "        #s_t = np.zeros((self.hidden_dim, input.shape[1]))\r\n",
        "        for i in range(self.num_lags):\r\n",
        "          s_t_tilde = np.tanh(np.matmul(H, Ws) + np.matmul(input[i,:,:], Us) + bs)\r\n",
        "          f_t = sigmoid(np.matmul(H, Wf) + np.matmul(input[i,:,:], Uf) + bf)\r\n",
        "          i_t = sigmoid(np.matmul(H, Wi) + np.matmul(input[i,:,:], Ui) + bi)\r\n",
        "          s_t = (f_t * s_t) + (i_t * s_t_tilde)\r\n",
        "          o_t = sigmoid(np.matmul(H, Wo) + np.matmul(input[i,:,:], Uo) + bo)\r\n",
        "          H = o_t * np.tanh(s_t)       \r\n",
        "        H = np.matmul(H, V) + c\r\n",
        "        return H\r\n",
        "    return _init, _apply\r\n",
        "\r\n",
        "  def loss(self, params, batch):\r\n",
        "    X, y = batch\r\n",
        "    y_pred = self.net_apply(params, X)\r\n",
        "    loss = np.mean((y - y_pred)**2)\r\n",
        "    return loss\r\n",
        "\r\n",
        "  # Define a compiled update step\r\n",
        "  @partial(jit, static_argnums=(0,))\r\n",
        "  def step(self, i, opt_state, batch):\r\n",
        "      params = self.get_params(opt_state)\r\n",
        "      g = grad(self.loss)(params, batch)\r\n",
        "      return self.opt_update(i, g, opt_state)\r\n",
        "\r\n",
        "  def data_stream(self, n, num_batches, batch_size):\r\n",
        "    rng = npr.RandomState(0)\r\n",
        "    while True:\r\n",
        "      perm = rng.permutation(n)\r\n",
        "      for i in range(num_batches):\r\n",
        "        batch_idx = perm[i*batch_size:(i+1)*batch_size]\r\n",
        "        yield self.X[:, batch_idx, :], self.Y[batch_idx, :]\r\n",
        "\r\n",
        "  def train(self, num_epochs = 100, batch_size = 64):   \r\n",
        "    n = self.X.shape[1]\r\n",
        "    num_complete_batches, leftover = divmod(n, batch_size)\r\n",
        "    num_batches = num_complete_batches + bool(leftover) \r\n",
        "    batches = self.data_stream(n, num_batches, batch_size)\r\n",
        "    pbar = trange(num_epochs)\r\n",
        "    for epoch in pbar:\r\n",
        "      for _ in range(num_batches):\r\n",
        "        batch = next(batches)\r\n",
        "        self.opt_state = self.step(next(self.itercount), self.opt_state, batch)\r\n",
        "      self.net_params = self.get_params(self.opt_state)\r\n",
        "      loss_value = self.loss(self.net_params, batch)\r\n",
        "      self.loss_log.append(loss_value)\r\n",
        "      pbar.set_postfix({'Loss': loss_value})\r\n",
        "\r\n",
        "  @partial(jit, static_argnums=(0,))\r\n",
        "  def predict(self, params, inputs):\r\n",
        "    Y_pred = self.net_apply(params, inputs)\r\n",
        "    return Y_pred"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHNfG0jIV5KI"
      },
      "source": [
        "# This will be the training data for the model using Analytical Data from the ODE as validation (beta assigned using MLP)\r\n",
        "\r\n",
        "rng_key = random.PRNGKey(0)\r\n",
        "noise = 0.0\r\n",
        "\r\n",
        "dataset = data_Analytical.T\r\n",
        "dataset = dataset + dataset.std(0)*noise*random.normal(rng_key, dataset.shape)\r\n",
        "\r\n",
        "# Use 2/3 of all data as training Data\r\n",
        "train_sizeL = int(len(dataset) * (4.0/5.0))\r\n",
        "train_data = dataset[0:train_sizeL,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuIorHvWWM1I"
      },
      "source": [
        "Determining the Optimal LSTM hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TawrHf00V7bO"
      },
      "source": [
        "def optimization(model2, lags, hidden, epochs, batchsize):\r\n",
        "\r\n",
        "  model2.train(num_epochs = epochs, batch_size = batchsize) # this is our model prediction with 100 epochs total\r\n",
        "\r\n",
        "  opt_params2 = model2.net_params\r\n",
        "  # One-step ahead prediction (normalized)\r\n",
        "  N, D = dataset.shape\r\n",
        "  pred2 = np.zeros((N-lags, D))\r\n",
        "  X_tmp =  model2.X[:,0:1,:]\r\n",
        "\r\n",
        "  for i in trange(N-lags):\r\n",
        "    pred2 = index_update(pred2, index[i:i+1], model2.net_apply(opt_params2, X_tmp))\r\n",
        "    X_tmp = index_update(X_tmp, index[:-1,:,:], X_tmp[1:,:,:])\r\n",
        "    X_tmp = index_update(X_tmp, index[-1,:,:], pred2[i])\r\n",
        "  # De-normalize predictions\r\n",
        "  pred2 = pred2*model2.std + model2.mean\r\n",
        "  error2 = np.linalg.norm(dataset[lags:] - pred2, 2)/np.linalg.norm(dataset[lags:], 2)\r\n",
        "  prediction_error = np.linalg.norm(dataset[])\r\n",
        "  return pred2, error2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E6C2NRoWJV4"
      },
      "source": [
        "epoch_tests = [10, 15, 25, 25, 25, 50, 50] # wide range of epoch testing (10-200)\r\n",
        "hidden_tests = [1, 2, 3, 4, 5, 10, 15, 20] # wide range of hidden dimensions (1-20)\r\n",
        "lag_tests = [1, 2, 3, 4, 5, 10, 15, 20] # wide range of lags (1-20)\r\n",
        "batch_sizes = [16, 32, 64] # wide range of batchsizes (16-64)\r\n",
        "\r\n",
        "hyper_coords = []\r\n",
        "preds = []\r\n",
        "errors = []\r\n",
        "losses = []\r\n",
        "\r\n",
        "counter = 0\r\n",
        "\r\n",
        "for hidden in hidden_tests:\r\n",
        "  for lag in lag_tests:\r\n",
        "    for batchsize in batch_sizes:\r\n",
        "      model2 = RNN(train_data, lag, hidden, rng_key)\r\n",
        "      total_epochs = 0\r\n",
        "      for epochs in epoch_tests:\r\n",
        "        total_epochs += epochs\r\n",
        "        pred, error = optimization(model2, lag, hidden, epochs, batchsize)\r\n",
        "        hyper_coord = (lag, hidden, total_epochs, batchsize)\r\n",
        "\r\n",
        "        x = model2.loss_log\r\n",
        "        hyper_coords.append(hyper_coord)\r\n",
        "        preds.append(pred)\r\n",
        "        errors.append(error)\r\n",
        "        losses.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwCNW8kSWU3a"
      },
      "source": [
        "def dataExtraction(ind):\r\n",
        "\r\n",
        "  pred2 = preds[ind]\r\n",
        "  lags = hyper_coords[ind][0]\r\n",
        "  epochs = hyper_coords[ind][2]\r\n",
        "  loss_log = losses[ind]\r\n",
        "\r\n",
        "  S_learned = []\r\n",
        "  I_learned = []\r\n",
        "  R_learned = []\r\n",
        "  D_learned = []\r\n",
        "  C_learned = []\r\n",
        "\r\n",
        "  for i in range(0, pred2.shape[0]):\r\n",
        "    S_learned.append(pred2[i][0])\r\n",
        "    I_learned.append(pred2[i][1])\r\n",
        "    R_learned.append(pred2[i][2])\r\n",
        "    D_learned.append(pred2[i][3])\r\n",
        "    C_learned.append(pred2[i][4])\r\n",
        "\r\n",
        "  plt.figure(figsize=(25,8), facecolor=\"w\")\r\n",
        "\r\n",
        "  Sl = np.asarray(S_learned, dtype=np.float32)\r\n",
        "  Il = np.asarray(I_learned, dtype=np.float32)\r\n",
        "  Rl = np.asarray(R_learned, dtype=np.float32)\r\n",
        "  Dl = np.asarray(D_learned, dtype=np.float32)\r\n",
        "  Cl = np.asarray(C_learned, dtype=np.float32)\r\n",
        "\r\n",
        "  plt.subplot(1,3,1)\r\n",
        "  plt.plot(Il, 'r-.', linewidth = 3, label = \"Currently Infected (I) - Prediction\")\r\n",
        "  plt.plot(Rl, 'r-*', linewidth = 3, label = \"Total Recovered (R) - Prediction\")\r\n",
        "  plt.plot(Dl, 'r-+', linewidth = 3, label = \"Total Deceased (D) - Prediction\")\r\n",
        "  plt.plot(Cl, 'r-', linewidth = 3, label = \"Cumulative Caseload (C) - Prediction\")\r\n",
        "\r\n",
        "  plt.plot(Ie[lags:], 'b-.', linewidth = 2, label = \"Currently Infected (I) - Exact\")\r\n",
        "  plt.plot(Re[lags:], 'b-*', linewidth = 2, label = \"Total Recovered (R) - Exact\")\r\n",
        "  plt.plot(De[lags:], 'b-+', linewidth = 2, label = \"Total Deceased (D) - Exact\")\r\n",
        "  plt.plot(Ce[lags:], 'b-', linewidth = 2, label = \"Cumulative Caseload (C) - Exact\")\r\n",
        "\r\n",
        "  plt.plot(CurrentlyInfected_data[train_size + lags:], 'r--', label = 'Currently Infected (I) - Data')\r\n",
        "  plt.plot(Recovered_data[train_size + lags:], 'g--', label = 'Recovered (R) - Data')\r\n",
        "  plt.plot(Deceased_data[train_size + lags:], 'k--', label = 'Deceased (D) - Data')\r\n",
        "  plt.plot(CumulativeCaseload_data[train_size + lags:], 'p--', label = 'Total Caseload (C) - Data')\r\n",
        "\r\n",
        "  plt.title(\"COVID-19 Dynamics in NJ\\n lags, epochs, hidden_dim, batchsize - {hypers}\".format(hypers = hyper_coords[ind]))\r\n",
        "  plt.xlabel(\"Time (days)\")\r\n",
        "  plt.ylabel(\"Number of Individuals\")\r\n",
        "  plt.legend()\r\n",
        "  plt.axvline(train_sizeL - lags)\r\n",
        "  \r\n",
        "  plt.subplot(1,3,2)\r\n",
        "  plt.plot(Sl, 'r-.', linewidth = 3, label = \"Susceptible (S)) - Prediction\")\r\n",
        "  plt.plot(Se[lags:], 'b-.', linewidth = 2, label = \"Susceptible (S) - Exact\")\r\n",
        "  plt.plot(Susceptible_data[train_size + lags:], 'k-.', linewidth = 1, label = \"Susceptible (S) - Data\")\r\n",
        "\r\n",
        "  plt.title(\"COVID-19 Susceptible Population in NJ\\n lags, epochs, hidden_dim, batchsize - {hypers}\".format(hypers = hyper_coords[ind]))\r\n",
        "  plt.xlabel(\"Time (days)\")\r\n",
        "  plt.ylabel(\"Number of Individuals\")\r\n",
        "  plt.legend()\r\n",
        "  plt.axvline(train_sizeL - lags)\r\n",
        "\r\n",
        "  plt.subplot(1,3,3)\r\n",
        "  plt.plot(loss_log)\r\n",
        "  plt.yscale('log')\r\n",
        "  plt.title(\"Loss over Time\\n lags, epochs, hidden_dim, batchsize - {hypers}\".format(hypers = hyper_coords[ind]))\r\n",
        "  plt.xlabel(\"Epochs\")\r\n",
        "  plt.ylabel(\"Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR9MyHC9WeqH"
      },
      "source": [
        "er = np.asarray(errors)\r\n",
        "ordered_er = np.sort(er)\r\n",
        "\r\n",
        "for i in range(0, int(0.1*len(errors))):\r\n",
        "  val = ordered_er[i]\r\n",
        "  ind = errors.index(val)\r\n",
        "  print(i)\r\n",
        "  dataExtraction(ind)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}